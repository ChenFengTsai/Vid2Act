defaults:

  logdir: null
  traindir: null
  evaldir: null
  video_dir: '/storage/ssd1/richtsai1103/iso_ted/log/metaworld/multi_5/mentor_open/dreamer_fixed-long_2/train_eps'
  pretrain_model_dir: null
  # offline_traindir: '/storage/ssd1/richtsai1103/iso_ted/log/metaworld/multi_5/mentor_open/dreamer_fixed-long_2/train_eps'
  # offline_evaldir: '/storage/ssd1/richtsai1103/iso_ted/log/metaworld/multi_5/mentor_open/dreamer_fixed-long_2/eval_eps'
  offline_traindir: ''
  offline_evaldir: ''
  
  # remember to modify this everytime you change model type when doing distillation
  # teacher_model_path: /home/richtsai1103/CRL/Vid2Act/logs/original_teacher/teacher_model.pt  # Required
  # vae_model_path: /home/richtsai1103/CRL/Vid2Act/logs/original_teacher/vae_model.pt       # Optional (but recommended)
  teacher_model_path: /home/richtsai1103/CRL/Vid2Act/logs/moe_teacher/teacher_model.pt  # Required
  vae_model_path: /home/richtsai1103/CRL/Vid2Act/logs/moe_teacher/vae_model.pt       # Optional (but recommended)
  teacher_encoder_mode: 'original_cnn'

  seed: 0
  steps: 1e7
  eval_every: 1e4
  log_every: 1e4
  reset_every: 0
  #gpu_growth: True
  device: 'cuda:1'
  precision: 16
  debug: False
  expl_gifs: False
  camera: corner2
  num_teachers: 6
  eval_num: 10
  is_adaptive: True
  source_tasks: null
  use_distill: True
  use_vae: False
  distill_weight: 1.0
  online_mode: True
  moe_temperature: '1.0'


  # Environment
  task: 'dmc_walker_walk'
  size: [64, 64]
  envs: 1
  action_repeat: 2
  time_limit: 1000
  grayscale: False
  prefill: 2500
  eval_noise: 0.0
  clip_rewards: 'identity'

  # Model
  dyn_cell: 'gru'
  dyn_hidden: 200
  dyn_deter: 200
  dyn_stoch: 50
  dyn_discrete: 0
  dyn_input_layers: 1
  dyn_output_layers: 1
  dyn_rec_depth: 1
  dyn_shared: False
  dyn_mean_act: 'none'
  dyn_std_act: 'sigmoid2'
  dyn_min_std: 0.1
  dyn_temp_post: True
  grad_heads: ['image', 'reward']
  units: 400
  reward_layers: 2
  discount_layers: 3
  value_layers: 3
  actor_layers: 4
  act: 'ELU'
  cnn_depth: 32
  encoder_kernels: [4, 4, 4, 4]
  decoder_kernels: [5, 5, 6, 6]
  decoder_thin: True
  value_head: 'normal'
  kl_scale: '1.0'
  kl_balance: '0.8'
  kl_free: '2.0'
  kl_forward: False
  pred_discount: False
  discount_scale: 1.0
  reward_scale: 1.0
  weight_decay: 0.0

  # Training
  batch_size: 50
  batch_length: 50
  train_every: 5
  train_steps: 1
  pretrain: 100
  model_lr: 3e-4
  value_lr: 8e-5
  actor_lr: 8e-5
  opt_eps: 1e-5
  grad_clip: 100
  value_grad_clip: 100
  actor_grad_clip: 100
  dataset_size: 0
  oversample_ends: False
  slow_value_target: True
  slow_actor_target: True
  slow_target_update: 100
  slow_target_fraction: 1
  opt: 'adam'

  # Behavior.
  discount: 0.99
  discount_lambda: 0.95
  imag_horizon: 15
  imag_gradient: 'dynamics'
  imag_gradient_mix: '0.1'
  imag_sample: True
  actor_dist: 'trunc_normal'
  actor_entropy: '1e-4'
  actor_state_entropy: 0.0
  actor_init_std: 1.0
  actor_min_std: 0.1
  actor_disc: 5
  actor_temp: 0.1
  actor_outscale: 0.0
  expl_amount: 0.0
  eval_state_mean: False
  collect_dyn_sample: True
  behavior_stop_grad: True
  value_decay: 0.0
  future_entropy: False

  # Exploration
  expl_behavior: 'greedy'
  expl_until: 0
  expl_extr_scale: 0.0
  expl_intr_scale: 1.0
  disag_target: 'stoch'
  disag_log: True
  disag_models: 10
  disag_offset: 1
  disag_layers: 4
  disag_units: 400
  disag_action_cond: False


dmlab:

  # General
  task: 'dmlab_rooms_watermaze'
  steps: 2e8
  eval_every: 1e5
  log_every: 1e4
  prefill: 50000
  dataset_size: 2e6
  pretrain: 0

  # Environment
  time_limit: 108000  # 30 minutes of game play.
  #grayscale: True
  action_repeat: 4
  eval_noise: 0.0
  train_every: 16
  train_steps: 1
  clip_rewards: 'tanh'

  # Model
  grad_heads: ['image', 'reward', 'discount']
  dyn_cell: 'gru_layer_norm'
  pred_discount: True
  cnn_depth: 48
  dyn_deter: 600
  dyn_hidden: 600
  dyn_stoch: 32
  dyn_discrete: 32
  reward_layers: 4
  discount_layers: 4
  value_layers: 4
  actor_layers: 4

  # Behavior
  actor_dist: 'onehot'
  actor_entropy: 'linear(3e-3,3e-4,2.5e6)'
  expl_amount: 0.0
  discount: 0.999
  imag_gradient: 'both'
  imag_gradient_mix: 'linear(0.1,0,2.5e6)'

  # Training
  discount_scale: 5.0
  reward_scale: 1
  weight_decay: 1e-6
  model_lr: 2e-4
  kl_scale: 0.1
  kl_free: 0.0
  actor_lr: 4e-5
  value_lr: 1e-4
  oversample_ends: True


atari:

  # General
  task: 'atari_pong'
  steps: 2e8
  eval_every: 1e5
  log_every: 1e4
  prefill: 50000
  dataset_size: 2e6
  pretrain: 0

  # Environment
  time_limit: 108000  # 30 minutes of game play.
  grayscale: True
  action_repeat: 4
  eval_noise: 0.0
  train_every: 16
  train_steps: 1
  clip_rewards: 'tanh'

  # Model
  grad_heads: ['image', 'reward', 'discount']
  dyn_cell: 'gru_layer_norm'
  pred_discount: True
  cnn_depth: 48
  dyn_deter: 600
  dyn_hidden: 600
  dyn_stoch: 32
  dyn_discrete: 32
  reward_layers: 4
  discount_layers: 4
  value_layers: 4
  actor_layers: 4

  # Behavior
  actor_dist: 'onehot'
  actor_entropy: 'linear(3e-3,3e-4,2.5e6)'
  expl_amount: 0.0
  discount: 0.999
  imag_gradient: 'both'
  imag_gradient_mix: 'linear(0.1,0,2.5e6)'

  # Training
  discount_scale: 5.0
  reward_scale: 1
  weight_decay: 1e-6
  model_lr: 2e-4
  kl_scale: 0.1
  kl_free: 0.0
  actor_lr: 4e-5
  value_lr: 1e-4
  oversample_ends: True

dmc:

  # General
  task: 'dmc_walker_walk'
  steps: 5e5
  eval_every: 3e3
  log_every: 3e3
  prefill: 2500
  dataset_size: 0
  pretrain: 100
  eval_num: 1
  num_teachers: 4
  source_tasks: ['cheetah_run', 'hopper_stand', 'walker_walk', 'walker_run']

  # Environment
  time_limit: 1000
  action_repeat: 2
  train_every: 5
  train_steps: 1

  # Model
  grad_heads: ['image', 'reward']
  dyn_cell: 'gru_layer_norm'
  pred_discount: False
  cnn_depth: 32
  dyn_deter: 200
  dyn_stoch: 50
  dyn_discrete: 0
  reward_layers: 2
  discount_layers: 3
  value_layers: 3
  actor_layers: 4

  # Behavior
  actor_dist: 'trunc_normal'
  expl_amount: 0.0
  actor_entropy: '1e-4'
  discount: 0.99
  imag_gradient: 'dynamics'
  imag_gradient_mix: 1.0

  # Training
  reward_scale: 2
  weight_decay: 0.0
  model_lr: 3e-4
  value_lr: 8e-5
  actor_lr: 8e-5
  opt_eps: 1e-5
  kl_free: '1.0'
  kl_scale: '1.0'


metaworld:

  task: metaworld_door_lock

  # General
  steps: 1.5e5
  eval_every: 6e3
  log_every: 2e3
  prefill: 3000
  dataset_size: 500000
  # pretrain: 3000
  eval_num: 5 # original 10
  num_teachers: 6 # teacher and expert are different, teacher are based on task number, expert does not

  # target_tasks: ['door_close', 'faucet_open', 'handle_press', 'plate_slide', 'reach_wall', 'window_close']
  source_tasks: ['button_press_topdown', 'door_open', 'drawer_close', 'peg_insert_side', 'pick_place', 'push']
  source_task_dirs: [
    '/storage/ssd1/richtsai1103/vid2act/pretrain/metaworld/pretrain_data/top50/dreamer_button-press-topdown',
    '/storage/ssd1/richtsai1103/vid2act/pretrain/metaworld/pretrain_data/top50/dreamer_door-open',
    '/storage/ssd1/richtsai1103/vid2act/pretrain/metaworld/pretrain_data/top50/dreamer_drawer-close',
    '/storage/ssd1/richtsai1103/vid2act/pretrain/metaworld/pretrain_data/top50/dreamer_peg-insert-side',
    '/storage/ssd1/richtsai1103/vid2act/pretrain/metaworld/pretrain_data/top50/dreamer_pick-place',
    '/storage/ssd1/richtsai1103/vid2act/pretrain/metaworld/pretrain_data/top50/dreamer_push'
  ]
  
  # source_tasks: [
  #   "door_open",
  #   "drawer_open",
  #   "window_open"
  # ]

  # source_task_dirs: [
  #   "/storage/ssd1/richtsai1103/iso_ted/log/metaworld/multi_5/mentor_open/dreamer_fixed-long_2/train_eps_by_task/task_0",
  #   "/storage/ssd1/richtsai1103/iso_ted/log/metaworld/multi_5/mentor_open/dreamer_fixed-long_2/train_eps_by_task/task_1",
  #   "/storage/ssd1/richtsai1103/iso_ted/log/metaworld/multi_5/mentor_open/dreamer_fixed-long_2/train_eps_by_task/task_2"
  # ]

  
  # Environment
  time_limit: 300
  action_repeat: 2
  train_every: 5
  train_steps: 1

  # Model
  grad_heads: ['image', 'reward']
  dyn_cell: 'gru_layer_norm'
  pred_discount: False
  cnn_depth: 32
  dyn_deter: 200
  dyn_stoch: 50
  dyn_discrete: 0
  reward_layers: 2
  discount_layers: 3
  value_layers: 3
  actor_layers: 4

  # ========== TEACHER MODEL ARCHITECTURE (FULL SIZE) ==========
  teacher_cnn_depth: 32       # Full original size
  teacher_dyn_hidden: 200     # Full original size
  teacher_dyn_deter: 200      # Full original size
  teacher_dyn_stoch: 50       # Full original size
 
  # Behavior
  actor_dist: 'trunc_normal'
  expl_amount: 0.0
  actor_entropy: '1e-4'
  # actor_entropy: '10e-4'
  discount: 0.99
  imag_gradient: 'dynamics'
  imag_gradient_mix: '1.0'

  # Training
  reward_scale: 2
  weight_decay: 0.0
  model_lr: 3e-4
  value_lr: 8e-5
  actor_lr: 8e-5
  opt_eps: 1e-5
  kl_free: '1.0'
  kl_scale: '1.0'

metaworld_teacher_moe_pretrain: # assume multi-head

  teacher_encoder_mode: 'moe'
  n_experts: 6
  use_orthogonal: True
  pretrain_iterations: 200000
  moe_temperature: '1.0'

metaworld_student_scale_down:
  
  # ========== SCALE DOWN VERSION ==========
  cnn_depth: 16              # SCALED DOWN from 32 → 16
  dyn_hidden: 50            # SCALED DOWN from 200 → 50
  units: 100                # SCALED DOWN from 400 → 100

  # ========== TEACHER MODEL ARCHITECTURE (FULL SIZE) ==========
  teacher_cnn_depth: 32       # Full original size
  teacher_dyn_hidden: 200     # Full original size
  teacher_dyn_deter: 200      # Full original size
  teacher_dyn_stoch: 50       # Full original size


debug:

  debug: True
  pretrain: 1
  prefill: 1
  train_steps: 1
  batch_size: 10
  batch_length: 20
